{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4 - CS348 Spring 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description** - In this assignment, you will run and analyze binary classification using K-nearest neighbors and multiclass classification using a fully connected neural network.\n",
    "\n",
    "**Getting Started** - You should complete the assignment using your own installation of Python 3 and the packages numpy, pandas, keras, matplotlib, and seaborn. Download the assignment from Moodle and unzip the file. This will create a directory with this file, 'HW04.ipynb'.\n",
    "\n",
    "**Deliverables** - The assignment has a single deliverable: this jupyter notebook file saved as a pdf. Please answer all coding and writing questions in the body of this file. Once all of the answers are complete, download the file by navigating the following menus: File -> Download as -> PDF via LaTeX. Submit the downloaded pdf file on gradescope. Alternatively, you can save the file as a pdf via the following: File -> Print Preview -> Print as pdf.\n",
    "\n",
    "**Data Sets** - In this assignment, you will two datasets from the sci-kit learn repository, one on breast cancer and one on handwritten digits.\n",
    "\n",
    "**Academic Honesty Statement** - Copying solutions from external sources (books, web pages, etc.) or other students is considered cheating. Sharing your solutions with other students is considered cheating. Posting your code to public repositories such as GitHub is also considered cheating. Any detected cheating will result in a grade of 0 on the assignment for all students involved, and potentially a grade of F in the course. \n",
    "\n",
    "This academic honesty statement does not restrict you from reading official documentation or using other web resources for understanding the syntax of python, related data science libraries, or properties of distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Do not import any other libraries other than those listed here. \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn as skl\n",
    "import seaborn as sns\n",
    "import keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import load_breast_cancer, load_digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1 - KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem you'll use a K-nearest neighbors model to classify whether a tumor is benign or malignant in a breast cancer dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data.\n",
    "data = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = data['data'].shape[0]\n",
    "\n",
    "index = [346, 446, 385,  90, 353, 333, 408, 398, 151,  11, 326, 164, 244,\n",
    "       252, 471, 208,  30, 135,  12, 472, 109, 263,  62, 504, 313, 266,\n",
    "       301, 558, 460, 213, 498, 526, 444,  10, 264, 239,  44, 561, 543,\n",
    "        83,  87,  32, 551, 516, 358, 218, 552, 507, 272, 141, 533,  98,\n",
    "       190, 337, 243, 149,  13, 319, 285, 513, 383, 167, 295, 506, 563,\n",
    "       303, 162, 507, 451, 264, 270, 332, 566, 385, 334, 562, 191, 241,\n",
    "       133, 268, 453, 126, 175,  19, 114, 406, 281, 310, 323,  48, 218,\n",
    "       401, 541, 510, 114, 533,  54, 374, 127, 321, 487,  89, 300, 178,\n",
    "       356,  89, 430, 333,  37, 471, 486, 301, 183, 107, 164,  31,  77,\n",
    "       163, 568, 359, 443, 508, 274, 324,  67, 147,  83, 332, 470,  98,\n",
    "        90, 468,  95, 477, 403, 164, 189, 489, 199, 432, 212,  90,  94,\n",
    "        86, 184, 319, 448, 382, 296, 323, 551, 363, 386, 273, 407, 254,\n",
    "        97, 274, 459, 560, 154, 515, 559, 281,  79, 134, 451,   9, 252,\n",
    "       485, 305, 245, 221, 219, 467,  37, 497, 186, 327, 311, 236, 397,\n",
    "       189, 468, 334, 158, 231, 100, 542,  40, 410, 481, 359, 304, 269,\n",
    "       267, 194, 144,  48,  22, 381, 288, 507, 530, 393, 207,  47, 551,\n",
    "       202, 387, 239,  16, 286, 280, 553,  80, 327,  66, 452, 508, 446,\n",
    "       418, 280, 426, 468, 233, 284,  51, 117,  40, 502, 410, 109,  81,\n",
    "       449, 420,  10, 112,  25, 484, 315, 424,  70,  86, 229, 518, 567,\n",
    "       131, 454, 422, 251, 108,  95, 154, 215, 325, 341, 414, 502, 324,\n",
    "       364,  54,  82,   1, 177, 442, 144,  86, 337, 538, 453,   0, 251,\n",
    "       481,  58, 451, 537, 364, 479,  44, 207, 382, 194, 487, 562,   3,\n",
    "       460, 128, 278, 121, 431, 466, 361, 411, 556, 408, 370, 216, 443,\n",
    "       124, 408,   9, 316, 267, 380,  29, 549, 481, 137, 439, 328, 381,\n",
    "       112, 239, 141, 227, 337, 568, 173, 252, 456, 450, 257, 173, 182,\n",
    "       241, 328, 489,  14, 318, 166, 191, 235,   6,   8,  43, 508, 318,\n",
    "       465,  68, 488, 156, 238,  30, 484,  52, 249, 107, 259,  29, 359,\n",
    "       567, 521, 346, 429, 361, 123,  64, 180, 241, 219, 196,  67, 230,\n",
    "       483, 341, 410, 129,  94, 320, 172, 446, 125, 113, 451, 214, 273,\n",
    "        87, 198, 217, 284, 565, 172, 328,  57, 251, 513, 463,  43, 334,\n",
    "        91, 265, 547, 195, 434, 280, 371, 254, 351, 167, 190, 281, 259,\n",
    "       212, 323, 206, 168,  33, 565, 234, 147, 277,  56, 262, 290, 233,\n",
    "       178, 153, 423, 134,  84, 146, 358,  85, 112,  27, 563, 447, 197,\n",
    "       388,  37, 294, 106,  86, 253, 192, 175, 344, 556, 553, 375,   3,\n",
    "       495,  21,  73, 329, 435, 316, 463, 165, 287, 319, 429, 170, 343,\n",
    "       379, 546, 209, 524,   8, 538, 483, 131, 284, 338, 460,  89, 370,\n",
    "        62, 214, 429,  26, 133,  22,  13, 358, 390, 315,  24, 288, 191,\n",
    "       307, 494, 459, 375, 248,  43, 461, 304, 414,  73, 277,  15, 442,\n",
    "       261, 353, 465, 271, 542, 152, 431, 538, 305, 345, 132, 440, 111,\n",
    "       520, 524, 366,  83, 379, 446, 472,  88,  11, 113, 366, 431, 160,\n",
    "       435, 430, 519, 207, 290, 372, 513, 108, 327, 122, 395, 183, 414,\n",
    "       269, 326, 329, 169, 384,   5, 522, 161, 213, 197, 532, 451, 446,\n",
    "       358,  53, 373, 321, 422, 408, 441, 422, 338, 489,  10, 521, 150,\n",
    "       288, 545, 341,  25, 510, 330, 365,  45, 335,  89]\n",
    "\n",
    "x = data['data'][index]\n",
    "y = data['target'][index]\n",
    "\n",
    "x_train = x[:int(n/2)]#(x[:int(n/2)] - x.min(axis=0))/x.sum(axis=0)\n",
    "y_train = y[:int(n/2)]\n",
    "x_val = x[int(n/2):]#(x[int(n/2):] - x.min(axis=0))/x.sum(axis=0)\n",
    "y_val = y[int(n/2):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.206e+01, 1.890e+01, 7.666e+01, 4.453e+02, 8.386e-02, 5.794e-02,\n",
       "       7.510e-03, 8.488e-03, 1.555e-01, 6.048e-02, 2.430e-01, 1.152e+00,\n",
       "       1.559e+00, 1.802e+01, 7.180e-03, 1.096e-02, 5.832e-03, 5.495e-03,\n",
       "       1.982e-02, 2.754e-03, 1.364e+01, 2.706e+01, 8.654e+01, 5.626e+02,\n",
       "       1.289e-01, 1.352e-01, 4.506e-02, 5.093e-02, 2.880e-01, 8.083e-02])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 (15 points)  \n",
    "Using sci-kit learn's `KNeighborsClassifier` class, fit a K-nearest neighbors classifier between features `x_train` and targets `y_train`. Without using `KNeighborsClassifier`'s `score()` method or `sklearn.metrics.accuracy_score`, compute the classification accuracy of the model on the training data, `x_train` and `y_train`, as well as the validation data, `x_val` and `y_val`.\n",
    "\n",
    "Note: The classification accuracy is defined as N_correct / N, where N_correct is the number of correctly classified instances and N is the total number of instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data:\n",
      "0.9330985915492958\n",
      "Validation data:\n",
      "0.9017543859649123\n"
     ]
    }
   ],
   "source": [
    "# Part 1 Solution\n",
    "\n",
    "# --- write code here ---\n",
    "knn=KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(x_train,y_train)\n",
    "#compute accuracy: N_correct/N_total\n",
    "def compute_accuracy(given,actual):\n",
    "    predictions=knn.predict(given)\n",
    "    n_correct=0\n",
    "    for i in range(0,len(actual)):\n",
    "        if(predictions[i]==actual[i]):\n",
    "            n_correct+=1\n",
    "    accuracy=n_correct/len(actual)\n",
    "    #print(\"N={}\".format(len(actual)))\n",
    "    #print(\"correct predictions={}\".format(n_correct))\n",
    "    #print(\"accuracy={}\".format(accuracy))\n",
    "    #print()\n",
    "    return accuracy\n",
    "print(\"Training data:\")\n",
    "print(compute_accuracy(x_train,y_train))\n",
    "print(\"Validation data:\")\n",
    "print(compute_accuracy(x_val,y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 (20 points)  \n",
    "For every value of k between 1 and 50, fit a K-nearest neighbors classifier between features `x_train` and targets `y_train`. For each model, compute the classification accuracy of the model on the training data, `x_train` and `y_train`, as well as the validation data, `x_val` and `y_val`. Make a scatterplot with values of k on the horizontal axis and the classification accuracy on the vertical axis, using colors to distinguish between the evaluations on training and validation data.\n",
    "\n",
    "Describe how and why classification accuracy differs between the two datasets when k=1. Which value of k should we choose? Justify your answer in 2-3 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Accuracy of Training/Validation data for k=1:50')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Part 2 Solution\n",
    "\n",
    "# --- write code here ---\n",
    "#compute accuracy for k values 1-50\n",
    "training_accuracy_list=[]\n",
    "validation_accuracy_list=[]\n",
    "accuracies={}\n",
    "for k in range(1,51):\n",
    "    knn=KNeighborsClassifier(n_neighbors=k) #compute for k neighbors\n",
    "    knn.fit(x_train,y_train)\n",
    "    #compute accuracy: N_correct/N_total\n",
    "    #training\n",
    "    training_accuracy_list.append(compute_accuracy(x_train,y_train))\n",
    "    #validation\n",
    "    validation_accuracy_list.append(compute_accuracy(x_val,y_val))\n",
    "    accuracies[k]=[]\n",
    "    accuracies[k].append(compute_accuracy(x_train,y_train))\n",
    "    accuracies[k].append(compute_accuracy(x_val,y_val))\n",
    "k_vals=range(1,51) #setup y_axis\n",
    "plt.scatter(k_vals,training_accuracy_list,label='Training') #training is blue\n",
    "plt.scatter(k_vals,validation_accuracy_list,label='Validation') #validation is orange\n",
    "plt.legend()\n",
    "plt.xlabel(\"K-value\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.title(\"Accuracy of Training/Validation data for k=1:50\")\n",
    "#print(validation_accuracy_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 Written Response\n",
    "\n",
    "_Type your written response here_\n",
    "\n",
    "Classification accuracy for k=1 on the training data has an accuracy value of 1.0, whereas the accuracy of k=1 on validation data is 0.9. This is reasonable because the training data has been trained using exactly the right answer, when k=1 the model has the highest variance possible, meaning that it will fit perfectly for the data it is trained on, thus the algorithm makes the correct prediction for all training data points. For validation data, the accuracy is lower since we have high variance, so different data sets will drastically affect results. \n",
    "\n",
    "I think that we should choose a value of K based solely on the accuracy of the validation data because that is the whole reason we are making the model in the first place - to classify unknown data samples. With this in mind, first we select the highest accuracy k-values, and in the case of a tie, the larger k-value is preferable because larger k-values reduce the effect of noise (class outliers) on classification. In other words, higher k-values decrease variance. Therefore, the ideal choice for K is 18 in this instance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 (15 points)  \n",
    "Consider two new hypothetical datasets, `x_train_inf` and `x_val_inf` and their corresponding labels `y_train_inf` and `y_val_inf`, each of which contain an infinite number of samples from the same data generating process as parts 1-3. Disregarding computational and storage constraints, would you expect a K-nearest neighbors model trained on the `x_train_inf` and `y_train_inf` to achieve a classification accuracy of 1.0 when evaluated on `x_val_inf` and `y_val_inf`? Justify your answer in 2-3 sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 Written Response\n",
    "\n",
    "_Type your written response here_\n",
    "\n",
    "\n",
    "If we choose to let k=1 then the KNN model will have the highest possible variance. In this case, the model will fit perfectly on data it is trained with. The usual drawback of high variance is that the model will vary drastically according to the data it is trained with. However, in this special case of infinite training samples, we need not worry about the model's variance, since it is being trained with all possible training data sets (infinitely large x_train_inf). Therefore, when we measure the model's accuracy for predicting validation data, we can be confident that it will be 1.0 because the model is perfectly fit for all possible data points.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 (5 points - Extra Credit)\n",
    "\n",
    "Your colleague claims that K-nearest neighbors is an inappropriate model for the raw data, and that the classifier could be improved by scaling each column of `x_train` to be between 0 and 1. Using your colleague's suggestion, train a new model and reproduce the plot from part 2. Explain how and why this data transformation changes the accuracy of the model.\n",
    "\n",
    "Hint: Whatever transformation you apply to `x_train` should also be applied to `x_val` when evaluating the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 4 Solution\n",
    "# --- write code here ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4 Written Response\n",
    "\n",
    "_Type your written response here_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2 - Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem you'll use a fully-connected neural network to classify a dataset of handwritten digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_data = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = digit_data['data'].shape[0]\n",
    "\n",
    "x = digit_data['data']\n",
    "y = digit_data['target']\n",
    "\n",
    "x_train = x[:int(n/2)]\n",
    "y_train = [keras.utils.to_categorical(y[:int(n/2)], num_classes=10)]\n",
    "x_val = x[int(n/2):]\n",
    "y_val = [keras.utils.to_categorical(y[int(n/2):], num_classes=10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 (30 points)  \n",
    "Write a function `NN_model(n)`, which returns a compiled keras neural network model with n fully connected layers, each with 64 nodes and a rectified linear unit activation function. After each fully connected layer add a dropout layer with a dropout parameter of 0.1. After the n layers, add a final fully connected layer with 10 nodes and a sigmoid activation. Compile your model with a categorical crossentropy loss, the adam optimization method, and classification accuracy as the only metric.\n",
    "\n",
    "Hint: You may find it helpful to use the Keras sequential model API. https://keras.io/getting-started/sequential-model-guide/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1 Solution\n",
    "# --- write code here ---\n",
    "num_cols=digit_data['data'].shape[1]\n",
    "def NN_model(n):\n",
    "    #return compiled KERAS neural net model \n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=num_cols, activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    for i in range(1,n-1):\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dropout(0.1))\n",
    "    model.add(Dense(10, activation='sigmoid'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 (20 points)  \n",
    "Train your neural network model on `x_train` and `y_train` with `n=1` for 50 epochs, a batch size of 100,  and `verbose=True`. Then evaluate your trained model on `x_val` and `y_val`. Is the classification accuracy monotonic increasing with the number of training steps? Explain why this relationship is/isn't monotonic in 2-3 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "898/898 [==============================] - 1s 1ms/step - loss: 3.9934 - acc: 0.1091\n",
      "Epoch 2/50\n",
      "898/898 [==============================] - 0s 21us/step - loss: 2.8356 - acc: 0.1069\n",
      "Epoch 3/50\n",
      "898/898 [==============================] - 0s 31us/step - loss: 2.2990 - acc: 0.1203\n",
      "Epoch 4/50\n",
      "898/898 [==============================] - 0s 30us/step - loss: 2.2262 - acc: 0.1203\n",
      "Epoch 5/50\n",
      "898/898 [==============================] - 0s 32us/step - loss: 2.1905 - acc: 0.1180\n",
      "Epoch 6/50\n",
      "898/898 [==============================] - 0s 32us/step - loss: 2.1446 - acc: 0.1437\n",
      "Epoch 7/50\n",
      "898/898 [==============================] - 0s 26us/step - loss: 2.1112 - acc: 0.1715\n",
      "Epoch 8/50\n",
      "898/898 [==============================] - 0s 33us/step - loss: 2.1108 - acc: 0.1548\n",
      "Epoch 9/50\n",
      "898/898 [==============================] - 0s 26us/step - loss: 2.0553 - acc: 0.1715\n",
      "Epoch 10/50\n",
      "898/898 [==============================] - 0s 27us/step - loss: 2.0069 - acc: 0.1793\n",
      "Epoch 11/50\n",
      "898/898 [==============================] - 0s 29us/step - loss: 1.9363 - acc: 0.2216\n",
      "Epoch 12/50\n",
      "898/898 [==============================] - 0s 32us/step - loss: 1.8237 - acc: 0.2773\n",
      "Epoch 13/50\n",
      "898/898 [==============================] - 0s 32us/step - loss: 1.6843 - acc: 0.2906\n",
      "Epoch 14/50\n",
      "898/898 [==============================] - 0s 30us/step - loss: 1.5217 - acc: 0.3018\n",
      "Epoch 15/50\n",
      "898/898 [==============================] - 0s 24us/step - loss: 1.3195 - acc: 0.4889\n",
      "Epoch 16/50\n",
      "898/898 [==============================] - 0s 31us/step - loss: 1.0603 - acc: 0.6748\n",
      "Epoch 17/50\n",
      "898/898 [==============================] - 0s 27us/step - loss: 0.7834 - acc: 0.7506\n",
      "Epoch 18/50\n",
      "898/898 [==============================] - 0s 28us/step - loss: 0.6388 - acc: 0.8085\n",
      "Epoch 19/50\n",
      "898/898 [==============================] - 0s 28us/step - loss: 0.5184 - acc: 0.8430\n",
      "Epoch 20/50\n",
      "898/898 [==============================] - 0s 30us/step - loss: 0.3778 - acc: 0.8786\n",
      "Epoch 21/50\n",
      "898/898 [==============================] - 0s 37us/step - loss: 0.3425 - acc: 0.8808\n",
      "Epoch 22/50\n",
      "898/898 [==============================] - 0s 31us/step - loss: 0.3281 - acc: 0.8853\n",
      "Epoch 23/50\n",
      "898/898 [==============================] - 0s 31us/step - loss: 0.2721 - acc: 0.9131\n",
      "Epoch 24/50\n",
      "898/898 [==============================] - 0s 27us/step - loss: 0.2195 - acc: 0.9165\n",
      "Epoch 25/50\n",
      "898/898 [==============================] - 0s 28us/step - loss: 0.2374 - acc: 0.9243\n",
      "Epoch 26/50\n",
      "898/898 [==============================] - 0s 27us/step - loss: 0.2158 - acc: 0.9310\n",
      "Epoch 27/50\n",
      "898/898 [==============================] - 0s 26us/step - loss: 0.2388 - acc: 0.9209\n",
      "Epoch 28/50\n",
      "898/898 [==============================] - 0s 32us/step - loss: 0.2070 - acc: 0.9321\n",
      "Epoch 29/50\n",
      "898/898 [==============================] - 0s 32us/step - loss: 0.1755 - acc: 0.9521\n",
      "Epoch 30/50\n",
      "898/898 [==============================] - 0s 23us/step - loss: 0.1574 - acc: 0.9510\n",
      "Epoch 31/50\n",
      "898/898 [==============================] - 0s 27us/step - loss: 0.1451 - acc: 0.9577\n",
      "Epoch 32/50\n",
      "898/898 [==============================] - 0s 34us/step - loss: 0.1889 - acc: 0.9399\n",
      "Epoch 33/50\n",
      "898/898 [==============================] - 0s 43us/step - loss: 0.1455 - acc: 0.9555\n",
      "Epoch 34/50\n",
      "898/898 [==============================] - 0s 28us/step - loss: 0.1230 - acc: 0.9588\n",
      "Epoch 35/50\n",
      "898/898 [==============================] - 0s 30us/step - loss: 0.1035 - acc: 0.9655\n",
      "Epoch 36/50\n",
      "898/898 [==============================] - 0s 33us/step - loss: 0.1181 - acc: 0.9644\n",
      "Epoch 37/50\n",
      "898/898 [==============================] - 0s 27us/step - loss: 0.1149 - acc: 0.9577\n",
      "Epoch 38/50\n",
      "898/898 [==============================] - 0s 31us/step - loss: 0.1194 - acc: 0.9532\n",
      "Epoch 39/50\n",
      "898/898 [==============================] - 0s 32us/step - loss: 0.0884 - acc: 0.9777\n",
      "Epoch 40/50\n",
      "898/898 [==============================] - 0s 24us/step - loss: 0.0845 - acc: 0.9644\n",
      "Epoch 41/50\n",
      "898/898 [==============================] - 0s 28us/step - loss: 0.1089 - acc: 0.9588\n",
      "Epoch 42/50\n",
      "898/898 [==============================] - 0s 27us/step - loss: 0.0929 - acc: 0.9710\n",
      "Epoch 43/50\n",
      "898/898 [==============================] - 0s 36us/step - loss: 0.0776 - acc: 0.9722\n",
      "Epoch 44/50\n",
      "898/898 [==============================] - 0s 34us/step - loss: 0.0746 - acc: 0.9733\n",
      "Epoch 45/50\n",
      "898/898 [==============================] - 0s 36us/step - loss: 0.0736 - acc: 0.9733\n",
      "Epoch 46/50\n",
      "898/898 [==============================] - 0s 34us/step - loss: 0.0816 - acc: 0.9699\n",
      "Epoch 47/50\n",
      "898/898 [==============================] - 0s 33us/step - loss: 0.0656 - acc: 0.9811\n",
      "Epoch 48/50\n",
      "898/898 [==============================] - 0s 32us/step - loss: 0.0599 - acc: 0.9811\n",
      "Epoch 49/50\n",
      "898/898 [==============================] - 0s 26us/step - loss: 0.0620 - acc: 0.9788\n",
      "Epoch 50/50\n",
      "898/898 [==============================] - 0s 28us/step - loss: 0.0575 - acc: 0.9822\n",
      "899/899 [==============================] - 0s 515us/step\n",
      "score=\n",
      "[0.33381242153376706, 0.9143492747864813]\n"
     ]
    }
   ],
   "source": [
    "# Part 2 Solution\n",
    "# --- write code here ---\n",
    "model=NN_model(n=1)\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=100, epochs=50, verbose=True)\n",
    "score = model.evaluate(x_val, y_val, batch_size=100)\n",
    "print(\"score=\")\n",
    "print(score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 Written Response\n",
    "\n",
    "_Type your written response here_\n",
    "\n",
    "The training accuracy is not monotonic increasing. We can see that the accuracy sometimes decreases between epochs. I think this is because the model is simulatenously adjusting itself to minimize loss (optimal weights for each parameter) as well as maximize accuracy (correct predictions of validation data).  \n",
    "\n",
    "Epoch 31/50\n",
    "898/898 [==============================] - 0s 27us/step - loss: 0.1451 - acc: 0.9577\n",
    "\n",
    "Epoch 32/50\n",
    "898/898 [==============================] - 0s 34us/step - loss: 0.1889 - acc: 0.9399"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 (5 points - Extra Credit)  \n",
    "For every integer value of n between 1 and 10, train a fully connected neural network using `NN_model(n)` on `x_train` and `y_train` and evaluate the trained model on `x_val` and `y_val`. Create a plot identical to the plot in Problem 1 part 2, except with n on the horizontal axis instead of k. Is this enough evidence to select a specific value for n? If not, what additional analysis would you suggest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "899/899 [==============================] - 0s 446us/step\n",
      "899/899 [==============================] - 0s 461us/step\n",
      "899/899 [==============================] - 0s 488us/step\n",
      "899/899 [==============================] - 0s 529us/step\n",
      "899/899 [==============================] - 1s 559us/step\n",
      "899/899 [==============================] - 1s 590us/step\n",
      "899/899 [==============================] - 1s 627us/step\n",
      "899/899 [==============================] - 1s 755us/step\n",
      "899/899 [==============================] - 1s 1ms/step\n",
      "899/899 [==============================] - 1s 861us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Accuracy of NN with n fully connected layers')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Part 3 Solution\n",
    "# --- write code here ---\n",
    "model_accuracy=[]\n",
    "def evaluate_models():\n",
    "    for i in range(1,11):\n",
    "        modelx=NN_model(i)\n",
    "        modelx.fit(x_train, y_train,\n",
    "              batch_size=100, epochs=50,verbose=False)\n",
    "        score = modelx.evaluate(x_val, y_val, batch_size=100)\n",
    "        model_accuracy.append(score[1])\n",
    "evaluate_models()\n",
    "plt.scatter(range(1,11),model_accuracy,label='validation accuracy')\n",
    "plt.xlabel(\"fully connected layers (n)\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.title(\"Accuracy of NN with n fully connected layers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 Written Response\n",
    "\n",
    "_Type your written response here_\n",
    "\n",
    "I think that additional analysis would be helpful in this case. Although we do evaluate how the number of layers effects model accuracy, it is also important to consider other hyperparameters for the model, such as the number of epochs. An epoch is when the entire dataset is passed forwards and backwards through the neural network one time, then the weights are adjusted in order to increase accuracy. As we go from a small number of epochs to a larger number, the model is at first underfitted, then optimal, and lastly overfitted for the data. 50 may not be the optimal number of epochs, so additional analysis could be analysing accuracy vs. number of epochs so that we can find the 'sweet spot' which is the optimal number of epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
